{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766aaa81-96e6-42dc-b29d-8216d2a7feec",
   "metadata": {},
   "source": [
    "# LLM GuardRails with Granite Guardian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffda72-22de-497c-a915-89d70bb10943",
   "metadata": {},
   "source": [
    "Without proper safeguards, Large Language Models (LLMs) can be **misused** or **exploited** to generate harmful content.  \n",
    "- Users could **bypass ethical constraints** by asking how to **steal money**, **hack accounts**, or **commit fraud**.  \n",
    "- AI models without guardrails may inadvertently **assist in illegal activities** or **spread misinformation**.  \n",
    "- Enterprises need **secure AI solutions** that ensure compliance, safety, and responsible usage.  \n",
    "- A **dedicated risk detection system** is essential to filter out harmful prompts **before they reach the LLM**.  \n",
    "\n",
    "## Granite Guardian\n",
    "\n",
    "Granite Guardian is a fine-tuned Granite 3 Instruct model designed to detect risks in prompts and responses. It can help with risk detection along many key dimensions catalogued in the [IBM AI Risk Atlas]().\n",
    "\n",
    "`Granite Guardian` enables application developers to screen user prompts and LLM responses for harmful content. These models are built on top of latest Granite family and are available at various platforms under the Apache 2.0 license:\n",
    "\n",
    "* Granite Guardian 3.1 8B : [HF](https://huggingface.co/ibm-granite/granite-guardian-3.1-8b)\n",
    "* Granite Guardian 3.1 2B : [HF](https://huggingface.co/ibm-granite/granite-guardian-3.1-2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022f3fb-ee50-40f2-b276-b8194668e49e",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ed2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"langchain==0.3.13\" \"langchain-openai==0.2.14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bb3f0f-40b5-49a6-b493-5e361db0113e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import warnings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9300dd-bf9b-44a9-810e-ea2dd11a1c93",
   "metadata": {},
   "source": [
    "## 2. Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec59fb9-d067-4a7d-b26d-304d72481909",
   "metadata": {},
   "source": [
    "### Inference Model Server Overview  \n",
    "\n",
    "This notebook utilizes two specialized LLMs:  \n",
    "\n",
    "- **Guardian Model:** [Granite-Guardian-3.1-2B](https://huggingface.co/ibm-granite/granite-guardian-3.1-2b)  \n",
    "  - Designed for risk detection and AI safety guardrails  \n",
    "- **Main LLM:** [Granite-3.1-8B-Instruct](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct)  \n",
    "  - Optimized for generating responses and handling user queries  \n",
    "\n",
    "These models work together to ensure AI-generated outputs are both **informative and safe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b908fd0-01dd-4ad2-b745-b3a4c56a7a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GUARDIAN_URL = os.getenv('GUARDIAN_URL')\n",
    "GUARDIAN_MODEL_NAME = \"granite3-guardian-2b\"\n",
    "GUARDIAN_API_KEY = os.getenv('GUARDIAN_API_KEY')\n",
    "\n",
    "LLM_URL = os.getenv('LLM_URL')\n",
    "LLM_MODEL_NAME = \"granite-3-8b-instruct\"\n",
    "LLM_API_KEY = os.getenv('LLM_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b2f3f-ac23-4531-984b-6e8357233992",
   "metadata": {},
   "source": [
    "## 3. Create the LLM instance\n",
    "\n",
    "**Why Use Two Models?**\n",
    "\n",
    "We initialize two separate LLMs to balance **safety** and **functionality**:  \n",
    "\n",
    "- **Guardian Model (Granite-Guardian-3.1-2B)**  \n",
    "  - Acts as a **safety layer** to detect risks before processing user inputs  \n",
    "  - Prevents harmful queries, misinformation, and improper function usage  \n",
    "\n",
    "- **Main LLM (Granite-3.1-8B-Instruct)**  \n",
    "  - Handles **actual query processing** and response generation  \n",
    "  - Provides informative, contextually relevant answers once input is deemed safe  \n",
    "\n",
    "This setup ensures that **potentially harmful inputs are blocked upfront**, while safe queries proceed seamlessly to the LLM for high-quality responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baa2b8-529d-455d-ad39-ef4a96dbaf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Guardian (Guardrails Model)\n",
    "guardian = ChatOpenAI(\n",
    "    openai_api_key=GUARDIAN_API_KEY,\n",
    "    openai_api_base=f\"{GUARDIAN_URL}/v1\",\n",
    "    model_name=GUARDIAN_MODEL_NAME,\n",
    "    temperature=0.01,\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "# Initialize LLM (LLM Model)\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=LLM_API_KEY,\n",
    "    openai_api_base=f\"{LLM_URL}/v1\",\n",
    "    model_name=LLM_MODEL_NAME,\n",
    "    temperature=0.01,\n",
    "    streaming=True,\n",
    "    #callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d69c62-13cc-4783-a3f5-ea239b564b5e",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions  \n",
    "\n",
    "These functions classify queries as **risky or safe** using the Guardian model:\n",
    "\n",
    "- **`check_risk function`** ‚Üí Flags risky queries (`True`) or allows safe ones (`False`).  \n",
    "- **`generate_response function`** ‚Üí Sends only safe queries to the main LLM for response generation.  \n",
    "\n",
    "This ensures **harmful inputs are blocked**, while valid queries receive high-quality AI responses.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c9db7e-c309-47a4-b2be-5a78735f3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tokens\n",
    "SAFE_TOKEN = \"No\"\n",
    "RISKY_TOKEN = \"Yes\"\n",
    "\n",
    "def check_risk(user_query):\n",
    "    \"\"\"\n",
    "    Step 1: Check risk using the Guardian model.\n",
    "    Returns True (risky) or False (safe).\n",
    "    \"\"\"\n",
    "    response = guardian.invoke([HumanMessage(content=user_query)])\n",
    "    risk_label = response.content.strip().lower()\n",
    "\n",
    "    return risk_label == RISKY_TOKEN.lower()\n",
    "\n",
    "def generate_response(user_query):\n",
    "    \"\"\"\n",
    "    Step 2: If input is safe, pass it to the main LLM.\n",
    "    \"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=user_query)])\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c3281-b244-4c20-a9a0-d7752bf0e41a",
   "metadata": {},
   "source": [
    "## 5. Query Processing Flow  \n",
    "\n",
    "This function **validates and processes user queries** by following a two-step pipeline:  \n",
    "\n",
    "1Ô∏è‚É£ **Risk Check (Guardian Model)**  \n",
    "   - If the query is **risky**, it is blocked with a üö´ warning.  \n",
    "   - If the query is **safe**, it proceeds to the LLM.  \n",
    "\n",
    "2Ô∏è‚É£ **Response Generation (Main LLM)**  \n",
    "   - Safe queries are sent to the LLM for a proper response.  \n",
    "   - The user receives either a **blocked notice** or an **LLM-generated answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1167cd6a-74cc-4141-8172-c1d0db8272e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_query):\n",
    "    \"\"\"\n",
    "    Full pipeline: \n",
    "    - Check if the query is risky.\n",
    "    - If safe, send it to the LLM.\n",
    "    - If risky, block the request.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Checking Risk for Query: '{user_query}'\")\n",
    "    \n",
    "    if check_risk(user_query):\n",
    "        final_response = \"üö´üîí This query violates safety guidelines. Blocked\"\n",
    "        print(\"‚ùå Risk detected! Query blocked for safety.\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Query is safe! \\n‚è© Forwarding to LLM...\")\n",
    "        final_response = generate_response(user_query)\n",
    "\n",
    "    return f\"\\nüéØ {final_response}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1eee8-d5e7-4b6c-9292-8b72cc55b9c4",
   "metadata": {},
   "source": [
    "## 6. Risky and Safe Example Queries\n",
    "\n",
    "### Example 1: Risky Query Handling  \n",
    "\n",
    "The system ensures **harmful queries** are blocked before reaching the main LLM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8d167e-b30c-42d9-820b-b457bb63609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking Risk for Query: 'How can I hurt someone?'\n",
      "‚ùå Risk detected! Query blocked for safety.\n",
      "\n",
      "üéØ üö´üîí This query violates safety guidelines. Blocked\n"
     ]
    }
   ],
   "source": [
    "risky_query = \"How can I hurt someone?\"  # Risky\n",
    "print(process_query(risky_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a11e7-c1f8-4dcf-b278-49af0e948a20",
   "metadata": {},
   "source": [
    "### Example 2: Safe Query Processing \n",
    "\n",
    "When a query is **safe**, it is forwarded to the main LLM for response generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c97281-51e0-45a0-b4eb-8df53a2ce175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking Risk for Query: 'What is the capital of Italy?'\n",
      "\n",
      "‚úÖ Query is safe! \n",
      "‚è© Forwarding to LLM...\n",
      "\n",
      "# Final Response: \n",
      "üéØ The capital of Italy is Rome. Established around 753 BC, Rome has a rich history and is renowned for its significant role in the Roman Empire, as well as its cultural and religious importance. Today, it is the capital city of Italy, serving as the political, diplomatic, and administrative center of the country. Rome is also famous for its historical landmarks, such as the Colosseum, the Roman Forum, and the Vatican City, which houses St. Peter's Basilica and the Sistine Chapel.\n"
     ]
    }
   ],
   "source": [
    "safe_query = \"What is the capital of Italy?\" # Safe\n",
    "final_output = process_query(safe_query)\n",
    "\n",
    "print(f\"\\n# Final Response: {final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425a878-2e9d-4f8f-aa48-420bef04d9d8",
   "metadata": {},
   "source": [
    "### Example 3: Risky Query Handling  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5942d902-a08d-4261-bbb5-2d5b615d7e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking Risk for Query: 'I want to kill all the humans. I am Bender!'\n",
      "‚ùå Risk detected! Query blocked for safety.\n",
      "\n",
      "üéØ üö´üîí This query violates safety guidelines. Blocked\n"
     ]
    }
   ],
   "source": [
    "risky_query = \"I want to kill all the humans. I am Bender!\"  # Risky\n",
    "print(process_query(risky_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce117a5-4e84-4ee0-93df-b91d79786d68",
   "metadata": {},
   "source": [
    "### Example 4: Safe Query Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c8639d-3a41-4dea-98ef-556c8d9bbcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking Risk for Query: 'What is the biggest mountain in the world?'\n",
      "\n",
      "‚úÖ Query is safe! \n",
      "‚è© Forwarding to LLM...\n",
      "\n",
      "# Final Response: \n",
      "üéØ The tallest mountain in the world is Mount Everest, with a peak at 8,848.86 meters (29,031.7 feet) above sea level, according to a 2020 revision of its height. It's located in the Himalayas on the border of Nepal and Tibet, China.\n"
     ]
    }
   ],
   "source": [
    "safe_query = \"What is the biggest mountain in the world?\" # Safe\n",
    "final_output = process_query(safe_query)\n",
    "\n",
    "print(f\"\\n# Final Response: {final_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
